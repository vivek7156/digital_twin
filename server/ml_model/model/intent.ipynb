{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/devel.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = [\n",
    "    {\n",
    "        \"sentence\": entry[\"sentence\"],\n",
    "        \"sentence_annotation\" : entry[\"sentence_annotation\"],\n",
    "        \"intent\" : entry[\"intent\"],\n",
    "        \"action\" : entry[\"action\"],\n",
    "        \"scenario\" : entry[\"scenario\"],\n",
    "    }\n",
    "    for entry in data\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "df[\"labels\"] = df[\"intent\"].astype(\"category\").cat.codes\n",
    "\n",
    "encoded_data = df[\"sentence\"].apply(lambda x: tokenizer(x, padding=\"max_length\", truncation=True, return_tensors=\"pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "num_labels = df['labels'].nunique()\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_annotation</th>\n",
       "      <th>intent</th>\n",
       "      <th>action</th>\n",
       "      <th>scenario</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>siri what is one american dollar in japanese yen</td>\n",
       "      <td>siri what is one [currency_name : american dol...</td>\n",
       "      <td>qa_currency</td>\n",
       "      <td>currency</td>\n",
       "      <td>qa</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how many unread emails do i have</td>\n",
       "      <td>how many unread emails do i have</td>\n",
       "      <td>email_query</td>\n",
       "      <td>query</td>\n",
       "      <td>email</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>order me chinese food</td>\n",
       "      <td>order me [food_type : chinese] food</td>\n",
       "      <td>takeaway_order</td>\n",
       "      <td>order</td>\n",
       "      <td>takeaway</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>does the nearby chinese restaurant do delivery</td>\n",
       "      <td>does the nearby [food_type : chinese] [busines...</td>\n",
       "      <td>takeaway_query</td>\n",
       "      <td>query</td>\n",
       "      <td>takeaway</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>remove pepper from my grocery list</td>\n",
       "      <td>remove pepper from my [list_name : grocery] list</td>\n",
       "      <td>lists_remove</td>\n",
       "      <td>remove</td>\n",
       "      <td>lists</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>can you give me local news on wayne county she...</td>\n",
       "      <td>can you give me local news on [news_topic : wa...</td>\n",
       "      <td>news_query</td>\n",
       "      <td>query</td>\n",
       "      <td>news</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2029</th>\n",
       "      <td>every light of room increase its intensity</td>\n",
       "      <td>[device_type : every light] of [house_place : ...</td>\n",
       "      <td>iot_hue_lightup</td>\n",
       "      <td>hue_lightup</td>\n",
       "      <td>iot</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>i would like some coffee now</td>\n",
       "      <td>i would like some coffee now</td>\n",
       "      <td>iot_coffee</td>\n",
       "      <td>coffee</td>\n",
       "      <td>iot</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>what is the population of los angeles</td>\n",
       "      <td>what is the population of [place_name : los an...</td>\n",
       "      <td>qa_factoid</td>\n",
       "      <td>factoid</td>\n",
       "      <td>qa</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>i need a taxi at eight tomorrow morning to tak...</td>\n",
       "      <td>i need a [transport_type : taxi] at [time : ei...</td>\n",
       "      <td>transport_taxi</td>\n",
       "      <td>taxi</td>\n",
       "      <td>transport</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2033 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  \\\n",
       "0      siri what is one american dollar in japanese yen   \n",
       "1                      how many unread emails do i have   \n",
       "2                                 order me chinese food   \n",
       "3        does the nearby chinese restaurant do delivery   \n",
       "4                    remove pepper from my grocery list   \n",
       "...                                                 ...   \n",
       "2028  can you give me local news on wayne county she...   \n",
       "2029         every light of room increase its intensity   \n",
       "2030                       i would like some coffee now   \n",
       "2031              what is the population of los angeles   \n",
       "2032  i need a taxi at eight tomorrow morning to tak...   \n",
       "\n",
       "                                    sentence_annotation           intent  \\\n",
       "0     siri what is one [currency_name : american dol...      qa_currency   \n",
       "1                      how many unread emails do i have      email_query   \n",
       "2                   order me [food_type : chinese] food   takeaway_order   \n",
       "3     does the nearby [food_type : chinese] [busines...   takeaway_query   \n",
       "4      remove pepper from my [list_name : grocery] list     lists_remove   \n",
       "...                                                 ...              ...   \n",
       "2028  can you give me local news on [news_topic : wa...       news_query   \n",
       "2029  [device_type : every light] of [house_place : ...  iot_hue_lightup   \n",
       "2030                       i would like some coffee now       iot_coffee   \n",
       "2031  what is the population of [place_name : los an...       qa_factoid   \n",
       "2032  i need a [transport_type : taxi] at [time : ei...   transport_taxi   \n",
       "\n",
       "           action   scenario  labels  \n",
       "0        currency         qa      51  \n",
       "1           query      email      15  \n",
       "2           order   takeaway      64  \n",
       "3           query   takeaway      65  \n",
       "4          remove      lists      37  \n",
       "...           ...        ...     ...  \n",
       "2028        query       news      43  \n",
       "2029  hue_lightup        iot      31  \n",
       "2030       coffee        iot      26  \n",
       "2031      factoid         qa      53  \n",
       "2032         taxi  transport      67  \n",
       "\n",
       "[2033 rows x 6 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.200831890106201\n",
      "Loss: 4.286402225494385\n",
      "Loss: 4.2241644859313965\n",
      "Loss: 4.284727573394775\n",
      "Loss: 4.27799129486084\n",
      "Loss: 4.363480091094971\n",
      "Loss: 4.417878150939941\n",
      "Loss: 4.27524471282959\n",
      "Loss: 4.272261142730713\n",
      "Loss: 4.406830310821533\n",
      "Loss: 4.35929536819458\n",
      "Loss: 4.390708923339844\n",
      "Loss: 4.300981521606445\n",
      "Loss: 4.318387031555176\n",
      "Loss: 4.247747421264648\n",
      "Loss: 4.457508563995361\n",
      "Loss: 4.210601806640625\n",
      "Loss: 4.198469638824463\n",
      "Loss: 4.258347511291504\n",
      "Loss: 4.353784084320068\n",
      "Loss: 4.230840682983398\n",
      "Loss: 4.169021129608154\n",
      "Loss: 4.229461669921875\n",
      "Loss: 4.31201696395874\n",
      "Loss: 4.296504974365234\n",
      "Loss: 4.093482494354248\n",
      "Loss: 4.088507175445557\n",
      "Loss: 4.117852687835693\n",
      "Loss: 4.426088333129883\n",
      "Loss: 4.1382670402526855\n",
      "Loss: 4.217711925506592\n",
      "Loss: 4.005492687225342\n",
      "Loss: 4.15946626663208\n",
      "Loss: 4.237598896026611\n",
      "Loss: 4.209317207336426\n",
      "Loss: 4.105384826660156\n",
      "Loss: 4.2845635414123535\n",
      "Loss: 4.219868183135986\n",
      "Loss: 4.212799072265625\n",
      "Loss: 3.9636118412017822\n",
      "Loss: 4.244555473327637\n",
      "Loss: 4.122553825378418\n",
      "Loss: 4.153406620025635\n",
      "Loss: 4.083793640136719\n",
      "Loss: 4.086638450622559\n",
      "Loss: 4.098498344421387\n",
      "Loss: 4.0837531089782715\n",
      "Loss: 4.06828498840332\n",
      "Loss: 4.060041427612305\n",
      "Loss: 4.145600318908691\n",
      "Loss: 4.261686325073242\n",
      "Loss: 4.228766441345215\n",
      "Loss: 4.007648468017578\n",
      "Loss: 4.182373523712158\n",
      "Loss: 3.971951961517334\n",
      "Loss: 3.961408853530884\n",
      "Loss: 4.153775691986084\n",
      "Loss: 4.0053791999816895\n",
      "Loss: 3.7624895572662354\n",
      "Loss: 4.109588146209717\n",
      "Loss: 4.158684730529785\n",
      "Loss: 4.074101448059082\n",
      "Loss: 3.895566463470459\n",
      "Loss: 3.9144396781921387\n",
      "Loss: 3.93778920173645\n",
      "Loss: 3.8589086532592773\n",
      "Loss: 3.8696625232696533\n",
      "Loss: 4.221208095550537\n",
      "Loss: 3.9961354732513428\n",
      "Loss: 4.010901927947998\n",
      "Loss: 3.8980700969696045\n",
      "Loss: 3.815491199493408\n",
      "Loss: 3.847280979156494\n",
      "Loss: 4.464859485626221\n",
      "Loss: 3.723104476928711\n",
      "Loss: 3.926147699356079\n",
      "Loss: 4.157925605773926\n",
      "Loss: 4.0001325607299805\n",
      "Loss: 3.72442626953125\n",
      "Loss: 3.6773390769958496\n",
      "Loss: 4.0284528732299805\n",
      "Loss: 3.9877145290374756\n",
      "Loss: 3.9097864627838135\n",
      "Loss: 4.205639839172363\n",
      "Loss: 3.5513253211975098\n",
      "Loss: 4.164603233337402\n",
      "Loss: 4.048723220825195\n",
      "Loss: 3.996901750564575\n",
      "Loss: 3.7329940795898438\n",
      "Loss: 3.9579343795776367\n",
      "Loss: 3.7444849014282227\n",
      "Loss: 3.932830333709717\n",
      "Loss: 3.7891345024108887\n",
      "Loss: 3.6598572731018066\n",
      "Loss: 3.6469521522521973\n",
      "Loss: 4.276010990142822\n",
      "Loss: 3.825498580932617\n",
      "Loss: 3.7377376556396484\n",
      "Loss: 3.57188081741333\n",
      "Loss: 3.565046548843384\n",
      "Loss: 3.9791338443756104\n",
      "Loss: 3.837963342666626\n",
      "Loss: 4.178524971008301\n",
      "Loss: 4.121001243591309\n",
      "Loss: 3.7993974685668945\n",
      "Loss: 3.4677135944366455\n",
      "Loss: 3.8993139266967773\n",
      "Loss: 3.8988218307495117\n",
      "Loss: 3.8027243614196777\n",
      "Loss: 3.753803014755249\n",
      "Loss: 3.4760499000549316\n",
      "Loss: 3.889085292816162\n",
      "Loss: 3.5434622764587402\n",
      "Loss: 3.7063992023468018\n",
      "Loss: 3.7335970401763916\n",
      "Loss: 3.471902847290039\n",
      "Loss: 3.7944259643554688\n",
      "Loss: 3.3465466499328613\n",
      "Loss: 3.732387065887451\n",
      "Loss: 3.7997519969940186\n",
      "Loss: 3.917814254760742\n",
      "Loss: 3.5239806175231934\n",
      "Loss: 4.05424690246582\n",
      "Loss: 3.923532247543335\n",
      "Loss: 3.7023684978485107\n",
      "Loss: 3.7448811531066895\n",
      "Loss: 3.703447103500366\n",
      "Loss: 3.4631803035736084\n",
      "Loss: 3.911874294281006\n",
      "Loss: 3.6176650524139404\n",
      "Loss: 3.6447551250457764\n",
      "Loss: 3.4154577255249023\n",
      "Loss: 3.4933159351348877\n",
      "Loss: 3.9533939361572266\n",
      "Loss: 3.915010690689087\n",
      "Loss: 3.7684998512268066\n",
      "Loss: 3.7693207263946533\n",
      "Loss: 3.880661725997925\n",
      "Loss: 3.4391565322875977\n",
      "Loss: 3.7219550609588623\n",
      "Loss: 3.761622428894043\n",
      "Loss: 3.6800360679626465\n",
      "Loss: 3.5552639961242676\n",
      "Loss: 3.1227519512176514\n",
      "Loss: 3.4681951999664307\n",
      "Loss: 3.411219596862793\n",
      "Loss: 3.4447219371795654\n",
      "Loss: 3.703986167907715\n",
      "Loss: 3.61375093460083\n",
      "Loss: 3.5064964294433594\n",
      "Loss: 3.397688388824463\n",
      "Loss: 3.608593225479126\n",
      "Loss: 3.760507822036743\n",
      "Loss: 2.8979403972625732\n",
      "Loss: 3.546010971069336\n",
      "Loss: 3.463024854660034\n",
      "Loss: 3.265526056289673\n",
      "Loss: 3.989572525024414\n",
      "Loss: 3.449655294418335\n",
      "Loss: 3.735013246536255\n",
      "Loss: 3.419856548309326\n",
      "Loss: 3.084834098815918\n",
      "Loss: 3.5423665046691895\n",
      "Loss: 3.5990567207336426\n",
      "Loss: 3.358785629272461\n",
      "Loss: 3.0023317337036133\n",
      "Loss: 3.346496820449829\n",
      "Loss: 3.625439167022705\n",
      "Loss: 3.2935283184051514\n",
      "Loss: 3.7711379528045654\n",
      "Loss: 4.028961181640625\n",
      "Loss: 3.506925582885742\n",
      "Loss: 3.4390382766723633\n",
      "Loss: 3.3100473880767822\n",
      "Loss: 4.0576605796813965\n",
      "Loss: 3.224198579788208\n",
      "Loss: 3.1783690452575684\n",
      "Loss: 3.4692208766937256\n",
      "Loss: 3.369917631149292\n",
      "Loss: 3.9779350757598877\n",
      "Loss: 3.9580447673797607\n",
      "Loss: 2.8492043018341064\n",
      "Loss: 2.9361164569854736\n",
      "Loss: 3.4183170795440674\n",
      "Loss: 3.5538270473480225\n",
      "Loss: 3.0439486503601074\n",
      "Loss: 3.5844361782073975\n",
      "Loss: 3.4800312519073486\n",
      "Loss: 3.047402858734131\n",
      "Loss: 3.2462422847747803\n",
      "Loss: 3.609672784805298\n",
      "Loss: 3.6741139888763428\n",
      "Loss: 3.243117094039917\n",
      "Loss: 2.925503969192505\n",
      "Loss: 2.983502149581909\n",
      "Loss: 3.245626449584961\n",
      "Loss: 3.106403112411499\n",
      "Loss: 3.610750675201416\n",
      "Loss: 3.0612387657165527\n",
      "Loss: 3.057713270187378\n",
      "Loss: 2.89374041557312\n",
      "Loss: 2.9487555027008057\n",
      "Loss: 3.3065757751464844\n",
      "Loss: 3.719176769256592\n",
      "Loss: 3.4730629920959473\n",
      "Loss: 2.7505760192871094\n",
      "Loss: 2.8540120124816895\n",
      "Loss: 3.352334976196289\n",
      "Loss: 3.094003677368164\n",
      "Loss: 3.4834671020507812\n",
      "Loss: 3.1289455890655518\n",
      "Loss: 3.007737159729004\n",
      "Loss: 3.6967058181762695\n",
      "Loss: 3.4165232181549072\n",
      "Loss: 3.3435111045837402\n",
      "Loss: 3.049494981765747\n",
      "Loss: 3.374960422515869\n",
      "Loss: 3.036693811416626\n",
      "Loss: 3.4836068153381348\n",
      "Loss: 3.3426733016967773\n",
      "Loss: 3.167948007583618\n",
      "Loss: 3.0301856994628906\n",
      "Loss: 2.805656909942627\n",
      "Loss: 3.1844162940979004\n",
      "Loss: 3.424556016921997\n",
      "Loss: 2.9758822917938232\n",
      "Loss: 3.3766133785247803\n",
      "Loss: 2.9014101028442383\n",
      "Loss: 2.545989990234375\n",
      "Loss: 3.2719717025756836\n",
      "Loss: 2.7826898097991943\n",
      "Loss: 3.1885693073272705\n",
      "Loss: 3.508963108062744\n",
      "Loss: 2.58734130859375\n",
      "Loss: 3.094921827316284\n",
      "Loss: 3.148724317550659\n",
      "Loss: 2.3366143703460693\n",
      "Loss: 2.8643240928649902\n",
      "Loss: 3.407449245452881\n",
      "Loss: 2.809187650680542\n",
      "Loss: 2.8812124729156494\n",
      "Loss: 3.2956411838531494\n",
      "Loss: 3.0951716899871826\n",
      "Loss: 2.233363389968872\n",
      "Loss: 2.948028564453125\n",
      "Loss: 2.61020827293396\n",
      "Loss: 3.6277060508728027\n",
      "Loss: 3.76674222946167\n",
      "Loss: 3.244802474975586\n",
      "Loss: 3.075246810913086\n",
      "Loss: 3.3726513385772705\n",
      "Loss: 3.3509275913238525\n",
      "Loss: 2.748875141143799\n",
      "Loss: 3.0845136642456055\n",
      "Loss: 1.982694149017334\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "\n",
    "num_labels = df[\"labels\"].nunique()\n",
    "\n",
    "df[\"labels\"] = df[\"labels\"] - df[\"labels\"].min()\n",
    "\n",
    "if not df[\"labels\"].between(0, num_labels - 1).all():\n",
    "    raise ValueError(f\"Labels out of range! Found: {df['labels'].unique()}\")\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "encoded_data = tokenizer(\n",
    "    df[\"sentence\"].tolist(),\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=64,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encoded_data[\"input_ids\"]\n",
    "attention_masks = encoded_data[\"attention_mask\"]\n",
    "labels = torch.tensor(df[\"labels\"].values, dtype=torch.long)\n",
    "\n",
    "batch_size = 8\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "dataloader = DataLoader(dataset, sampler=RandomSampler(dataset), batch_size=batch_size)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for batch in dataloader:\n",
    "    b_input_ids, b_attention_mask, b_labels = [t.to(device) for t in batch]\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    b_labels = b_labels.to(torch.long)\n",
    "\n",
    "    outputs = model(b_input_ids, attention_mask=b_attention_mask, labels=b_labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"processed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calendar_set\n"
     ]
    }
   ],
   "source": [
    "def predict_intent(text):\n",
    "    model.eval()\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    if \"labels\" not in df.columns:\n",
    "        raise KeyError(\"The 'labels' column is missing from the DataFrame.\")\n",
    "\n",
    "    if \"intent\" not in df.columns:\n",
    "        print(\"⚠️ Warning: 'intent' column not found, using default labels.\")\n",
    "        df[\"intent\"] = df[\"labels\"].astype(str)\n",
    "\n",
    "    intent_mapping = dict(zip(df[\"labels\"].unique(), df[\"intent\"].unique()))\n",
    "\n",
    "    intent_label = intent_mapping.get(predicted_class, \"Unknown Intent\")\n",
    "\n",
    "    return intent_label\n",
    "\n",
    "print(predict_intent(\"Could you put \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([51, 15, 64, 65, 37, 70,  6, 58, 16,  2, 17, 21, 52,  8, 20,  0, 47,\n",
       "       40, 25, 62, 69, 46, 38, 55, 13, 63, 43, 44, 53, 67, 48, 59, 35, 49,\n",
       "       11, 27, 56,  3,  7, 66, 68, 54, 50, 26, 28, 45, 36, 41, 60,  1,  9,\n",
       "       14, 29,  5,  4, 33, 19, 24, 12, 57, 31, 42, 32, 30, 23, 10, 39, 18,\n",
       "       61, 34, 22], dtype=int8)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"labels\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved in ./bert_intent_model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "save_directory = \"./bert_intent_model\"\n",
    "\n",
    "# Create directory if it does not exist\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved in {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "load_directory = \"./bert_intent_model\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(load_directory)\n",
    "\n",
    "# Load model\n",
    "model = BertForSequenceClassification.from_pretrained(load_directory)\n",
    "\n",
    "# Move model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'general_quirky'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_intent(\"Hey\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
